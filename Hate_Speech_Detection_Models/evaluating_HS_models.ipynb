{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0b7b1057",
   "metadata": {},
   "source": [
    "## Google's Perspective API\n",
    "\n",
    "https://developers.perspectiveapi.com/s/docs-sample-requests?language=en_US\n",
    "\n",
    "About the architecture: https://developers.perspectiveapi.com/s/about-the-api-model-cards?language=en_US\n",
    "\n",
    "Attributes:\n",
    "https://developers.perspectiveapi.com/s/about-the-api-attributes-and-languages?language=en_US\n",
    "\n",
    "\"IDENTITY_ATTACK\":\n",
    "\n",
    "Negative or hateful comments targeting someone because of their identity.\n",
    "\n",
    "**Threshold:**\n",
    "Researchers should consider which application of Perspective best matches their intent. For social science researchers using Perspective to study harassment, we recommend experimenting with thresholds of 0.7 or 0.9, similar to typical moderation use cases. By comparison, machine learning researchers using Perspective to filter potentially toxic content from their data should use higher thresholds, like 0.9 or 0.95, as low thresholds increase the potential for bias. Another researcher might analyze all comments with scores 0.7 or higher when researching false positives in machine learning models. ( https://developers.perspectiveapi.com/s/about-the-api-score?language=en_US )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0bc3ff3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from googleapiclient import discovery\n",
    "import json\n",
    "import time\n",
    "import numpy as np\n",
    "\n",
    "API_KEY = 'AIzaSyB3SOPV2_Ft9DZOY2hOo7xVEirOWe88_1Q'\n",
    "\n",
    "client = discovery.build(\n",
    "  \"commentanalyzer\",\n",
    "  \"v1alpha1\",\n",
    "  developerKey=API_KEY,\n",
    "  discoveryServiceUrl=\"https://commentanalyzer.googleapis.com/$discovery/rest?version=v1alpha1\",\n",
    "  static_discovery=False,\n",
    ")\n",
    "\n",
    "def google_perspective_predict(sent):\n",
    "  analyze_request = {\n",
    "      'comment': { 'text': sent },\n",
    "      'requestedAttributes': {'IDENTITY_ATTACK': {}},\n",
    "      'languages': [\"en\"],\n",
    "      }\n",
    "  response = client.comments().analyze(body=analyze_request).execute()\n",
    "  return response#[\"attributeScores\"][\"IDENTITY_ATTACK\"][\"summaryScore\"][\"value\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecd9cc39",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "score = google_perspective_predict('They carry bundles of sticks on their backs like dogs, and they are so ugly that one would think they were born from the devil himself.')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aae06107",
   "metadata": {},
   "source": [
    "## Pysentimiento\n",
    "\n",
    "Paper: https://arxiv.org/pdf/2106.09462\n",
    "\n",
    "Model: Similarly, BERTweet (Nguyen et al, 2020)\n",
    "is a RoBERTa model trained on approximately\n",
    "850 million English tweets\n",
    "\n",
    "\n",
    "Dataset: https://aclanthology.org/S19-2007.pdf\n",
    "https://github.com/pysentimiento/pysentimiento/tree/master"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d753b05e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install pysentimiento\n",
    "#!pip install opencv-python-headless\n",
    "#!pip install -U transformers\n",
    "from pysentimiento import create_analyzer\n",
    "hate_speech_analyzer = create_analyzer(task=\"hate_speech\", lang=\"en\")\n",
    "\n",
    "def pysent_predict(sent):\n",
    "  result = hate_speech_analyzer.predict(sent)\n",
    "  hatefullness = result.probas[\"hateful\"]\n",
    "  return hatefullness"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e1fc0e1",
   "metadata": {},
   "source": [
    "## Huggingface models\n",
    "\n",
    "We investigate the four most popular up-to-date (most downloads in the last month) Hate Speech detection models uploaded to Huggingface.\n",
    "\n",
    "Filters:\n",
    "\n",
    "*   Task = Text classification\n",
    "*   Language = English\n",
    "\n",
    "Models chosen on the 15th of August 2024 and the listed models are at the time of access the only models on huggingface having more than 1000 downloads in the last month.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb2837bc",
   "metadata": {},
   "source": [
    "### 1. facebook/roberta-hate-speech-dynabench-r4-target\n",
    "https://huggingface.co/facebook/roberta-hate-speech-dynabench-r4-target\n",
    "\n",
    "Paper: https://arxiv.org/pdf/2012.15761"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40a869b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe_fb_roberta = pipeline(\"text-classification\", model=\"facebook/roberta-hate-speech-dynabench-r4-target\")\n",
    "\n",
    "# Convert the output into a number in between 0 and 1 (0 signaling nonhate, 1 signaling hate)\n",
    "def fb_roberta_predict_score(sent):\n",
    "  result = pipe_fb_roberta(sent)\n",
    "  print(result)\n",
    "  if result[0]['label'] == 'nothate':\n",
    "    return 1 - result[0]['score']\n",
    "  else:\n",
    "    return result[0]['score']\n",
    "\n",
    "\n",
    "def fb_roberta_predict_label(sent):\n",
    "  result = pipe_fb_roberta(sent)\n",
    "  return \"non-hateful\" if result[0]['label'] == \"nothate\" else \"hateful\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c7ed552",
   "metadata": {},
   "outputs": [],
   "source": [
    "sent = 'Your moral character must be not only pure, but, like Caesars wife, unsuspected.'\n",
    "#sent = ['setr']*600\n",
    "score = fb_roberta_predict_score(sent)\n",
    "label = fb_roberta_predict_label(sent)\n",
    "score, label"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f51a4c0",
   "metadata": {},
   "source": [
    "### 2. Hate-speech-CNERG/english-abusive-MuRIL\n",
    "https://huggingface.co/Hate-speech-CNERG/english-abusive-MuRIL (abusive)\n",
    "\n",
    "Paper: https://arxiv.org/abs/2204.12543\n",
    "\n",
    "\n",
    "*   LABEL_0 :-> Normal\n",
    "*   LABEL_1 :-> Abusive\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e78cb282",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe_cnerg_abusive = pipeline(\"text-classification\", model=\"Hate-speech-CNERG/english-abusive-MuRIL\")\n",
    "\n",
    "# Convert the output into a number in between 0 and 1 (0 signaling nonhate, 1 signaling hate)\n",
    "def cnerg_abusive_predict_score(sent):\n",
    "  result = pipe_cnerg_abusive(sent)\n",
    "  if result[0]['label'] == 'LABEL_0':\n",
    "    return 1 - result[0]['score']\n",
    "  else:\n",
    "    return result[0]['score']\n",
    "\n",
    "def cnerg_abusive_predict_label(sent):\n",
    "  result = pipe_cnerg_abusive(sent)\n",
    "  return \"non-hateful\" if result[0]['label'] == \"LABEL_0\" else \"hateful\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97711d2a",
   "metadata": {},
   "source": [
    "### 3. Hate-speech-CNERG/bert-base-uncased-hatexplain\n",
    "https://huggingface.co/Hate-speech-CNERG/bert-base-uncased-hatexplain\n",
    "\n",
    "Paper & Dataset: https://ojs.aaai.org/index.php/AAAI/article/view/17745\n",
    "\n",
    "Base Model: Bert\n",
    "\n",
    "Labels:\n",
    "\n",
    "*   Normal\n",
    "*   Offensive\n",
    "*   Hate Speech\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3428644c",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe_cnerg_hatexplain = pipeline(\"text-classification\", model=\"Hate-speech-CNERG/bert-base-uncased-hatexplain\")\n",
    "\n",
    "# Convert the output into a number in between 0 and 1 (0 signaling nonhate, 1 signaling hate)\n",
    "def cnerg_hatexplain_predict_score(sent):\n",
    "  result = pipe_cnerg_hatexplain(sent)\n",
    "  if result[0]['label'] == 'normal':\n",
    "    return 1 - result[0]['score']\n",
    "  else:\n",
    "    return result[0]['score']\n",
    "\n",
    "# Assuming offensive sentences overlap with hateful sentences\n",
    "def cnerg_hatexplain_predict_label(sent):\n",
    "  result = pipe_cnerg_hatexplain(sent)\n",
    "  return \"non-hateful\" if result[0]['label'] == \"normal\" else \"hateful\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2dea362d",
   "metadata": {},
   "source": [
    "### 4. Hate-speech-CNERG/dehatebert-mono-english\n",
    "https://huggingface.co/Hate-speech-CNERG/dehatebert-mono-english\n",
    "\n",
    "Paper: https://arxiv.org/abs/2004.06465"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4317183a",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe_cnerg_dehatebert = pipeline(\"text-classification\", model=\"Hate-speech-CNERG/dehatebert-mono-english\")\n",
    "\n",
    "def cnerg_dehatebert_predict_label(sent):\n",
    "  result = pipe_cnerg_dehatebert(sent)\n",
    "  return \"non-hateful\" if result[0]['label'] == \"NON_HATE\" else \"hateful\"\n",
    "\n",
    "\n",
    "def cnerg_dehatebert_predict_score(sent):\n",
    "  result = pipe_cnerg_dehatebert(sent)\n",
    "  if result[0]['label'] == 'NON_HATE':\n",
    "    return 1 - result[0]['score']\n",
    "  else:\n",
    "    return result[0]['score']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54eb3c40",
   "metadata": {},
   "source": [
    "### 5. IMSyPP/hate_speech_en\n",
    "https://huggingface.co/IMSyPP/hate_speech_en\n",
    "\n",
    "Paper: https://link.springer.com/chapter/10.1007/978-3-031-08974-9_54\n",
    "\n",
    "#### Hate speech type\n",
    "At the speech type level, you can choose between four categories:\n",
    "0. **Appropriate** - no target (leave the \"target\" category blank)\n",
    "1. **Inappropriate** (contains terms that are obscene, vulgar; but the text is not directed at\n",
    "any person specifically) - has no target (leave the “target” category blank)\n",
    "2. **Offensive** (including offensive generalization, contempt, dehumanization, indirect\n",
    "offensive remarks)\n",
    "3. **Violent** (author threatens, indulges, desires or calls for physical violence against a\n",
    "target; it also includes calling for, denying or glorifying war crimes and crimes against\n",
    "humanity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "422981ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe_imsypp = pipeline(\"text-classification\", model=\"IMSyPP/hate_speech_en\")\n",
    "\n",
    "def imsypp_predict_label(sent):\n",
    "  result = pipe_imsypp(sent)\n",
    "  return \"non-hateful\" if result[0]['label'] == \"LABEL_0\" or result[0]['label'] == \"LABEL_1\" else \"hateful\"\n",
    "\n",
    "\n",
    "def imsypp_predict_score(sent):\n",
    "  result = pipe_imsypp(sent)\n",
    "  return result[0]['score']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b6c7e26",
   "metadata": {},
   "source": [
    "### 6. cardiffnlp/twitter-roberta-large-hate-latest\n",
    "https://huggingface.co/cardiffnlp/twitter-roberta-large-hate-latest\n",
    "\n",
    "Paper: https://arxiv.org/abs/2310.14757\n",
    "\n",
    "Base model: https://huggingface.co/cardiffnlp/twitter-roberta-large-2022-154m\n",
    "\n",
    "Fine-tune dataset: https://huggingface.co/datasets/cardiffnlp/super_tweeteval\n",
    "\n",
    "#### Labels\n",
    "* **hate_gender**\n",
    "* **hate_race**\n",
    "* **hate_sexuality**\n",
    "* **hate_religion**\n",
    "* **hate_origin**\n",
    "* **hate_disability**\n",
    "* **hate_age**\n",
    "* **not_hate**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "353663cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe_cardiff_roberta = pipeline(\"text-classification\", model=\"cardiffnlp/twitter-roberta-large-hate-latest\")\n",
    "\n",
    "# Function to get score for hate speech classification\n",
    "def cardiff_roberta_predict_score(sent):\n",
    "    result = pipe_cardiff_roberta(sent)\n",
    "    return result[0]['score']\n",
    "\n",
    "# Function to get label for hate speech classification\n",
    "def cardiff_roberta_predict_label(sent):\n",
    "    result = pipe_cardiff_roberta(sent)\n",
    "    return \"non-hateful\" if result[0]['label'] == \"not_hate\" else \"hateful\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8599744a",
   "metadata": {},
   "source": [
    "### 7. badmatr11x/distilroberta-base-offensive-hateful-speech-text-multiclassification\n",
    "https://huggingface.co/datasets/badmatr11x/hate-offensive-speech\n",
    "\n",
    "distilroberta base from HF finetuned on https://huggingface.co/datasets/badmatr11x/hate-offensive-speech\n",
    "\n",
    "https://huggingface.co/badmatr11x/distilroberta-base-offensive-hateful-speech-text-multiclassification/blob/main/README.md\n",
    "\n",
    "#### Labels\n",
    "0. **HATE-SPEECH**\n",
    "1. **OFFENSIVE-LANGUAGE** -> also hateful content in dataset\n",
    "2. **NEITHER**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24f67e32",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe_badmatrix = pipeline(\"text-classification\", model=\"badmatr11x/distilroberta-base-offensive-hateful-speech-text-multiclassification\")\n",
    "\n",
    "# Function to get score for hate speech classification\n",
    "def badmatrix_predict_score(sent):\n",
    "    result = pipe_badmatrix(sent)\n",
    "    return result[0]['score']\n",
    "\n",
    "# Function to get label for hate speech classification\n",
    "def badmatrix_predict_label(sent):\n",
    "    result = pipe_badmatrix(sent)\n",
    "    return \"non-hateful\" if result[0]['label'] == \"NEITHER\" else \"hateful\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d566f7e",
   "metadata": {},
   "source": [
    "### 8. tomh/toxigen_hatebert\n",
    "https://huggingface.co/tomh/toxigen_hatebert\n",
    "\n",
    "Paper: https://arxiv.org/pdf/2203.09509\n",
    "\n",
    "**Does not work**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89072f34",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the tomh/toxigen_hatebert model via pipeline\n",
    "pipe_toxigen = pipeline(\"text-classification\", model=\"tomh/toxigen_hatebert\")\n",
    "\n",
    "# Function to get score for hate speech classification\n",
    "def pipe_toxigen_predict_score(sent):\n",
    "    result = pipe_toxigen(sent)\n",
    "    return result[0]['score']\n",
    "\n",
    "# Function to get label for hate speech classification\n",
    "def pipe_toxigen_predict_label(sent):\n",
    "    result = pipe_toxigen(sent)\n",
    "    return \"non-hateful\" if result[0]['label'] == \"LABEL_1\" else \"hateful\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
