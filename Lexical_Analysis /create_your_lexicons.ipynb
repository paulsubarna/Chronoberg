{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "eafc20bb",
   "metadata": {},
   "source": [
    "## Create Your own lexicons based on user requirements\n",
    "\n",
    "\n",
    "There could be two ways to create your own lexicons.\n",
    "\n",
    "- Training your own word2vec models \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c61f827",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "data = torch.load('/app/src/ChronoBerg/cade/data_json/NRC_VAD_valence_v2.pt')\n",
    "\n",
    "\n",
    "positives = []\n",
    "\n",
    "n_w = ['enjoy', 'happiest', 'hugs', 'success']\n",
    "n_w = ['abundant', 'enjoy', 'hugs', 'jolly', 'laughter', 'liking', 'lucky', 'marvel', 'merry', 'respectful']\n",
    "n_w = ['shit', 'afraid', 'angered', 'annihilation','bankruptcy','betray','stabbed','strangulation','suicidal','chaos']\n",
    "n_w = ['asylum', 'coronary', 'depressive', 'germs', 'heartbreak', 'homeless', 'malfeasance', 'punk', 'sanctimonious', 'senile', 'unustified', 'weird']\n",
    "n_w = ['bloomers', 'destiny', 'dunk', 'febrile', 'infatuation', 'karma', 'outing','repertoire','sanitation','stockbroker','technology','tweak']\n",
    "n_w= ['redoubled', 'gayety', 'carelessness']\n",
    "for w_ in tqdm(n_w):\n",
    "    positives.append(w_)\n",
    "valence = {}\n",
    "for w_ in positives:\n",
    "    #if valence.get[w_] is None:\n",
    "    valence[w_] = []\n",
    "print(\"Number of words:\", len(positives))\n",
    "\n",
    "for n_ in tqdm(positives):\n",
    "    try:\n",
    "        top_10= slice_one.wv.most_similar(n_, topn=100)\n",
    "        res= []\n",
    "        click = 0\n",
    "        tri = 0 \n",
    "        for t_ in top_10:\n",
    "            w_ = t_[0]\n",
    "            tri += 1\n",
    "            try:\n",
    "                res.append(float(data[w_]))\n",
    "                click += 1\n",
    "                if click == 20:\n",
    "                    break\n",
    "            except:\n",
    "                pass #print(f\"Word not found: {w_}\")\n",
    "        \n",
    "        if len(res) == 0:\n",
    "            valence[n_].append('None')\n",
    "        else:\n",
    "            valence[n_].append(round(np.mean(res),2))\n",
    "        #clicks[n_].append(click)\n",
    "        #tries[n_].append(tri)\n",
    "    except:\n",
    "        valence[n_].append('None')\n",
    "\n",
    "    try:\n",
    "        top_10= slice_two.wv.most_similar(n_, topn=100)\n",
    "        res= []\n",
    "        click = 0\n",
    "        tri = 0 \n",
    "        for t_ in top_10:\n",
    "            w_ = t_[0]\n",
    "            tri += 1\n",
    "            try:\n",
    "                res.append(float(data[w_]))\n",
    "                click += 1\n",
    "                if click == 20:\n",
    "                    break\n",
    "            except:\n",
    "                pass #print(f\"Word not found: {w_}\")\n",
    "        \n",
    "        if len(res) == 0:\n",
    "            valence[n_].append('None')\n",
    "        else:\n",
    "            valence[n_].append(round(np.mean(res),2))\n",
    "        #clicks[n_].append(click)\n",
    "        #tries[n_].append(tri)\n",
    "    except:\n",
    "        valence[n_].append('None')\n",
    "\n",
    "    try:\n",
    "        top_10= slice_three.wv.most_similar(n_, topn=100)\n",
    "        res= []\n",
    "        click = 0\n",
    "        tri = 0 \n",
    "        for t_ in top_10:\n",
    "            w_ = t_[0]\n",
    "            tri += 1\n",
    "            try:\n",
    "                res.append(float(data[w_]))\n",
    "                click += 1\n",
    "                if click == 20:\n",
    "                    break\n",
    "            except:\n",
    "                pass #print(f\"Word not found: {w_}\")\n",
    "        \n",
    "        if len(res) == 0:\n",
    "            valence[n_].append('None')\n",
    "        else:\n",
    "            valence[n_].append(round(np.mean(res),2))\n",
    "        #clicks[n_].append(click)\n",
    "        #tries[n_].append(tri)\n",
    "    except:\n",
    "        valence[n_].append('None')\n",
    "\n",
    "    try:\n",
    "        top_10= slice_four.wv.most_similar(n_, topn=100)\n",
    "        res= []\n",
    "        click = 0\n",
    "        tri = 0 \n",
    "        for t_ in top_10:\n",
    "            w_ = t_[0]\n",
    "            tri += 1\n",
    "            try:\n",
    "                res.append(float(data[w_]))\n",
    "                click += 1\n",
    "                if click == 20:\n",
    "                    break\n",
    "            except:\n",
    "                pass #print(f\"Word not found: {w_}\")\n",
    "        \n",
    "        if len(res) == 0:\n",
    "            valence[n_].append('None')\n",
    "        else:\n",
    "            valence[n_].append(round(np.mean(res),2))\n",
    "        #clicks[n_].append(click)\n",
    "        #tries[n_].append(tri)\n",
    "    except:\n",
    "        valence[n_].append('None')\n",
    "\n",
    "    try:\n",
    "        top_10= slice_five.wv.most_similar(n_, topn=100)\n",
    "        res= []\n",
    "        click = 0\n",
    "        tri = 0 \n",
    "        for t_ in top_10:\n",
    "            w_ = t_[0]\n",
    "            tri += 1\n",
    "            try:\n",
    "                res.append(float(data[w_]))\n",
    "                click += 1\n",
    "                if click == 5:\n",
    "                    break\n",
    "            except:\n",
    "                pass #print(f\"Word not found: {w_}\")\n",
    "        \n",
    "        if len(res) == 0:\n",
    "            valence[n_].append('None')\n",
    "        else:\n",
    "            valence[n_].append(round(np.mean(res),2))\n",
    "        #clicks[n_].append(click)\n",
    "        #tries[n_].append(tri)\n",
    "    except:\n",
    "        valence[n_].append('None')\n",
    "valence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ab9769c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "data = torch.load('/app/src/ChronoBerg/cade/data_json/NRC_VAD_valence_v2.pt')\n",
    "\n",
    "\n",
    "#data= data.sort_values(by=['val'], ascending=False)\n",
    "#n_w = data.word.values\n",
    "positives = []\n",
    "#num_words = torch.load('/app/src/ChronoBerg/cade/valence_chrono.pt')\n",
    "#n_w = list(set(num_words))[args.count*50000: (args.count+1)*50000]\n",
    "n_w = ['applause', 'abundant', 'hugs', 'success']\n",
    "#n_w = ['abundant', 'enjoy', 'hugs', 'jolly', 'laughter', 'liking', 'lucky', 'marvel', 'merry', 'respectful']\n",
    "#n_w = ['shit', 'afraid', 'angered', 'annihilation','bankruptcy','betray','stabbed','strangulation','suicidal','chaos']\n",
    "#n_w = ['asylum', 'coronary', 'depressive', 'germs', 'heartbreak', 'homeless', 'malfeasance', 'punk', 'sanctimonious', 'senile', 'unustified', 'weird']\n",
    "#n_w = ['bloomers', 'destiny', 'dunk', 'febrile', 'infatuation', 'karma', 'outing','repertoire','sanitation','stockbroker','technology','tweak']\n",
    "#n_w= ['hugs', 'applause', 'abundant']\n",
    "for w_ in tqdm(n_w):\n",
    "    positives.append(w_)\n",
    "valence = {}\n",
    "for w_ in positives:\n",
    "    #if valence.get[w_] is None:\n",
    "    valence[w_] = []\n",
    "print(\"Number of words:\", len(positives))\n",
    "num=5\n",
    "for n_ in tqdm(positives):\n",
    "    try:\n",
    "        top_10= slice_one.wv.most_similar(n_, topn=num)\n",
    "        res= []\n",
    "        click = 0\n",
    "        tri = 0 \n",
    "        for t_ in top_10:\n",
    "            w_ = t_[0]\n",
    "            tri += 1\n",
    "            try:\n",
    "                res.append(float(data[w_]))\n",
    "                click += 1\n",
    "                \n",
    "            except:\n",
    "                pass #print(f\"Word not found: {w_}\")\n",
    "        \n",
    "        if len(res) == 0:\n",
    "            valence[n_].append('None')\n",
    "        else:\n",
    "            valence[n_].append(round(np.mean(res),2))\n",
    "        #clicks[n_].append(click)\n",
    "        #tries[n_].append(tri)\n",
    "    except:\n",
    "        valence[n_].append('None')\n",
    "\n",
    "    try:\n",
    "        top_10= slice_two.wv.most_similar(n_, topn=num)\n",
    "        res= []\n",
    "        click = 0\n",
    "        tri = 0 \n",
    "        for t_ in top_10:\n",
    "            w_ = t_[0]\n",
    "            tri += 1\n",
    "            try:\n",
    "                res.append(float(data[w_]))\n",
    "                click += 1\n",
    "\n",
    "            except:\n",
    "                pass #print(f\"Word not found: {w_}\")\n",
    "        \n",
    "        if len(res) == 0:\n",
    "            valence[n_].append('None')\n",
    "        else:\n",
    "            valence[n_].append(round(np.mean(res),2))\n",
    "        #clicks[n_].append(click)\n",
    "        #tries[n_].append(tri)\n",
    "    except:\n",
    "        valence[n_].append('None')\n",
    "\n",
    "    try:\n",
    "        top_10= slice_three.wv.most_similar(n_, topn=num)\n",
    "        res= []\n",
    "        click = 0\n",
    "        tri = 0 \n",
    "        for t_ in top_10:\n",
    "            w_ = t_[0]\n",
    "            tri += 1\n",
    "            try:\n",
    "                res.append(float(data[w_]))\n",
    "                click += 1\n",
    "\n",
    "            except:\n",
    "                pass #print(f\"Word not found: {w_}\")\n",
    "        \n",
    "        if len(res) == 0:\n",
    "            valence[n_].append('None')\n",
    "        else:\n",
    "            valence[n_].append(round(np.mean(res),2))\n",
    "        #clicks[n_].append(click)\n",
    "        #tries[n_].append(tri)\n",
    "    except:\n",
    "        valence[n_].append('None')\n",
    "\n",
    "    try:\n",
    "        top_10= slice_four.wv.most_similar(n_, topn=num)\n",
    "        res= []\n",
    "        click = 0\n",
    "        tri = 0 \n",
    "        for t_ in top_10:\n",
    "            w_ = t_[0]\n",
    "            tri += 1\n",
    "            try:\n",
    "                res.append(float(data[w_]))\n",
    "                click += 1\n",
    "\n",
    "            except:\n",
    "                pass #print(f\"Word not found: {w_}\")\n",
    "        \n",
    "        if len(res) == 0:\n",
    "            valence[n_].append('None')\n",
    "        else:\n",
    "            valence[n_].append(round(np.mean(res),2))\n",
    "        #clicks[n_].append(click)\n",
    "        #tries[n_].append(tri)\n",
    "    except:\n",
    "        valence[n_].append('None')\n",
    "\n",
    "    try:\n",
    "        top_10= slice_five.wv.most_similar(n_, topn=num)\n",
    "        res= []\n",
    "        click = 0\n",
    "        tri = 0 \n",
    "        for t_ in top_10:\n",
    "            w_ = t_[0]\n",
    "            tri += 1\n",
    "            try:\n",
    "                res.append(float(data[w_]))\n",
    "                click += 1\n",
    "\n",
    "            except:\n",
    "                pass #print(f\"Word not found: {w_}\")\n",
    "        \n",
    "        if len(res) == 0:\n",
    "            valence[n_].append('None')\n",
    "        else:\n",
    "            valence[n_].append(round(np.mean(res),2)) \n",
    "        #clicks[n_].append(click)\n",
    "        #tries[n_].append(tri)\n",
    "    except:\n",
    "        valence[n_].append('None')\n",
    "valence"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
