{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "76364253",
   "metadata": {},
   "source": [
    "## Determining Affective Polarity of Sentences in ChronoBerg & Comparisons with hate-check models\n",
    "\n",
    "In this notebook, we will walk through the steps to determine the affective polarities of sentences (negative/positive) along with the connotations acquired using hate-check tools such as Perspective API, RoBERTa, and OpenAI moderation tool. \n",
    "\n",
    "For Perspective API and OpenAI moderation tool, you need to pass your own API keys\n",
    "\n",
    "The dataset and lexicons are available at the Huggingface: [CHRONOBERG](https://huggingface.co/datasets/sdp56/ChronoBerg/tree/main)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "826ac045",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger_eng to\n",
      "[nltk_data]     /root/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger_eng is already up-to-\n",
      "[nltk_data]       date!\n",
      "/usr/local/lib/python3.10/dist-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "### Import necessary libraries\n",
    "import torch\n",
    "import json\n",
    "import pandas as pd\n",
    "from collections import defaultdict\n",
    "import math\n",
    "import nltk\n",
    "import numpy as np\n",
    "from googleapiclient import discovery\n",
    "import time\n",
    "import itertools\n",
    "from tqdm import tqdm\n",
    "import re\n",
    "from openai import OpenAI\n",
    "import os\n",
    "from nltk.tokenize import sent_tokenize\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "nltk.download('punkt_tab')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('averaged_perceptron_tagger_eng')\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import wordnet\n",
    "#from nltk.tokenize import word_tokenize\n",
    "import argparse\n",
    "#!pip install --upgrade transformers\n",
    "import transformers\n",
    "from transformers import pipeline\n",
    "os.chdir('/app/src/Chronoberg/')\n",
    "## Load Helper functions to load dataset\n",
    "from Dataset_statistics.load_data import load_data, preprocess_text, extract_sentence_splits_by_year, extract_text_by_year\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b0100fb",
   "metadata": {},
   "source": [
    "### Load the dataset and Valence lexicons "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7bdd529d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading lexicons...\n",
      "Loaded 87360 words from 1750 lexicon\n",
      "Loaded 133886 words from 1800 lexicon\n",
      "Loaded 181955 words from 1850 lexicon\n",
      "Loaded 199535 words from 1900 lexicon\n",
      "Loaded 85000 words from 1950 lexicon\n",
      "All lexicons loaded.\n"
     ]
    }
   ],
   "source": [
    "print(\"Loading lexicons...\")\n",
    "\n",
    "path_lexicons = '/app/src/ChronoBerg/cade/new_lexicons/'\n",
    "\n",
    "valence_1750 = torch.load(path_lexicons + 'valence_dic_1750_new.pt', weights_only=False)\n",
    "valence_1800 = torch.load(path_lexicons + 'valence_dic_1800_new.pt', weights_only=False)\n",
    "valence_1850 = torch.load(path_lexicons + 'valence_dic_1850_new.pt', weights_only=False)\n",
    "valence_1900 = torch.load(path_lexicons + 'valence_dic_1900_new.pt', weights_only=False)\n",
    "valence_1950 = torch.load(path_lexicons + 'valence_dic_1950_new.pt', weights_only=False)\n",
    "print(f\"Loaded {len(valence_1750)} words from 1750 lexicon\")\n",
    "print(f\"Loaded {len(valence_1800)} words from 1800 lexicon\")\n",
    "print(f\"Loaded {len(valence_1850)} words from 1850 lexicon\")\n",
    "print(f\"Loaded {len(valence_1900)} words from 1900 lexicon\")\n",
    "print(f\"Loaded {len(valence_1950)} words from 1950 lexicon\")\n",
    "\n",
    "print(\"All lexicons loaded.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "39a7c382",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading the dataset...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "249it [00:47,  5.19it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data loaded\n",
      "data sorted\n"
     ]
    }
   ],
   "source": [
    "print(\"Loading the dataset...\")\n",
    "\n",
    "\n",
    "data_dict = load_data(data_path= '/app/src/ChronoBerg/cade/data_json/pg_books_historic.jsonl')\n",
    "\n",
    "### Extract sentences and preprocess them\n",
    "sents, years = extract_sentence_splits_by_year(year=[1800], data_dict=data_dict)\n",
    "sents = preprocess_text(sents)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7082f16e",
   "metadata": {},
   "source": [
    "- Define Helper functions: \n",
    "    1. calculate_valence: Use to assign a single valence score for a group of tokens\n",
    "    - Split a sentences into different clauses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f7ec899e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_valence(tokens, file, negation_words):\n",
    "    total_sc = []\n",
    "    #print(tokens)\n",
    "    for tok in tokens:\n",
    "        try: \n",
    "            if tok in negation_words:\n",
    "                scores= -0.3\n",
    "            else:\n",
    "                scores = float(file[tok])\n",
    "        except:\n",
    "            scores = 0.0\n",
    "        if scores != 0.0:\n",
    "            total_sc.append(scores)\n",
    "    if total_sc != []:\n",
    "        #print(total_sc)\n",
    "        #print(total_sc)\n",
    "        return np.mean(total_sc)\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "def split_into_clauses(text):\n",
    "    # Split clauses using simple punctuation-based heuristic\n",
    "    clauses = re.split(r'[;,]|(?:\\sbut\\s)|(?:\\band\\b)', text, flags=re.IGNORECASE)\n",
    "    return [clause.strip() for clause in clauses if clause.strip()]\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "negation_words = [\"no\", \"not\", \"n't\", \"never\", \"none\", \"nobody\",\n",
    "    \"nothing\", \"neither\", \"nowhere\", \"hardly\",\n",
    "    \"scarcely\", \"barely\", \"without\"\n",
    "]\n",
    "stopwords_ = list(set(stopwords.words('english')) - set(negation_words))\n",
    "def get_sentence_score(sentence, lexicon, negation_words, stopwords=stopwords_):\n",
    "    for text in tqdm(sentence):\n",
    "\n",
    "        clauses =  split_into_clauses(text)\n",
    "\n",
    "        #print(clauses)\n",
    "        sc_ = []\n",
    "        for clause in clauses:\n",
    "            word_list= nltk.tokenize.word_tokenize(clause.lower())\n",
    "            pos_tags = nltk.pos_tag(word_list)\n",
    "            sentiment_words= []\n",
    "            for word, pos in pos_tags:\n",
    "                if pos.startswith(('JJ', 'VB')):\n",
    "                    sentiment_words.append(word)\n",
    "                elif word in list(lexicon.keys()):\n",
    "                    sentiment_words.append(word)\n",
    "                elif word in negation_words:\n",
    "                    sentiment_words.append(word)\n",
    "                elif pos.startswith('RB'):\n",
    "                    sentiment_words.append(lemmatizer.lemmatize(word, pos='a'))\n",
    "            #sentiment_words = [word for word, pos in pos_tags if pos.startswith(('JJ', 'RB', 'VB'))  ]\n",
    "            #sentiment_words = [word for word, pos in pos_tags if pos.startswith(('JJ', 'RB', 'VB'))  ]\n",
    "\n",
    "            if sentiment_words == []:\n",
    "            #    #continue\n",
    "                sentiment_words = word_list\n",
    "            tokens = [word for word in sentiment_words if word.isalpha()]\n",
    "\n",
    "            tokens = [token for token in tokens if token not in stopwords]\n",
    "\n",
    "\n",
    "            scores_ =[]\n",
    "            if tokens == []:\n",
    "                tokens = word_list\n",
    "                tokens = [token for token in tokens if token not in stopwords]\n",
    "\n",
    "            valence= calculate_valence(tokens, lexicon, negation_words) \n",
    "            if valence is not None:\n",
    "                sc_.append(valence)\n",
    "    return min(sc_) if sc_ else None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4af0e98e",
   "metadata": {},
   "source": [
    "#### Now's lets see how to calculate a valence score for a sentence "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "cd43e9c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 26.18it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Valence score for the sentence during 1750s: -0.262\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "## pass your sentence here to analyze\n",
    "your_sentence  = ['At your age, I would no more wish you to be a Cato than a Clodius']\n",
    "\n",
    "### Set the lexicon to the specified time period\n",
    "lexicon = valence_1750\n",
    "\n",
    "score = get_sentence_score(your_sentence, lexicon, negation_words)\n",
    "print(f\"The Valence score for the sentence during 1750s: {score}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f70277b",
   "metadata": {},
   "source": [
    "## Let's us detemine the connotation of a sentence using Hate-Check Tools"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef2ba010",
   "metadata": {},
   "source": [
    "- RoBERTA + Perspective API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "4cbd999d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda:0\n"
     ]
    }
   ],
   "source": [
    "\n",
    "API_KEY = 'AIzaSyB3SOPV2_Ft9DZOY2hOo7xVEirOWe88_1Q'\n",
    "\n",
    "pipe_fb_roberta = pipeline(\"text-classification\", model=\"facebook/roberta-hate-speech-dynabench-r4-target\")\n",
    "\n",
    "# Convert the output into a number in between 0 and 1 (0 signaling nonhate, 1 signaling hate)\n",
    "def fb_roberta_predict_score(sent):\n",
    "  result = pipe_fb_roberta(sent)\n",
    "  print(result)\n",
    "  if result[0]['label'] == 'nothate':\n",
    "    return 1 - result[0]['score']\n",
    "  else:\n",
    "    return result[0]['score']\n",
    "\n",
    "\n",
    "def fb_roberta_predict_label(sent):\n",
    "  result = pipe_fb_roberta(sent)\n",
    "  return \"non-hateful\" if result[0]['label'] == \"nothate\" else \"hateful\"\n",
    "\n",
    "client = discovery.build(\n",
    "  \"commentanalyzer\",\n",
    "  \"v1alpha1\",\n",
    "  developerKey=API_KEY,\n",
    "  discoveryServiceUrl=\"https://commentanalyzer.googleapis.com/$discovery/rest?version=v1alpha1\",\n",
    "  static_discovery=False,\n",
    ")\n",
    "\n",
    "def google_perspective_predict(sent):\n",
    "  analyze_request = {\n",
    "      'comment': { 'text': sent },\n",
    "      'requestedAttributes': {'IDENTITY_ATTACK': {}},\n",
    "      'languages': [\"en\"],\n",
    "      }\n",
    "  response = client.comments().analyze(body=analyze_request).execute()\n",
    "  return response[\"attributeScores\"][\"IDENTITY_ATTACK\"][\"summaryScore\"][\"value\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "3911555e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Perspective API score: 0.022590388\n",
      "------------ Google Perspective API Result ------------\n",
      "The sentence is classified as: Non_hateful: ✅\n"
     ]
    }
   ],
   "source": [
    "def google_perspective_roberta_full_response(sent, threshold_perspective=0.5):\n",
    "    label = fb_roberta_predict_label(sent)\n",
    "    if label == \"hateful\":\n",
    "        perspective_score = google_perspective_predict(sent)\n",
    "        print(f\"Perspective API score: {perspective_score}\")\n",
    "    else:\n",
    "        return \"✅\"\n",
    "    if perspective_score >= threshold_perspective:\n",
    "        return \"Hateful: 🚩\"\n",
    "    else:\n",
    "        return \"Non_hateful: ✅\"\n",
    "\n",
    "your_sentence  = ['At your age, I would no more wish you to be a Cato than a Clodius']\n",
    "result = google_perspective_roberta_full_response(your_sentence[0])\n",
    "print(\"------------ Google Perspective API Result ------------\")\n",
    "print(f\"The sentence is classified as: {result}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "487a3825",
   "metadata": {},
   "source": [
    "### OpenAI Moderation Tool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acd6ee14",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------ OpenAI Moderation Tool Result ------------\n",
      "The sentence is classified as: not flagged text: At your age, I would no more wish you to be a Cato than a Clodius and response: non_hate\n"
     ]
    }
   ],
   "source": [
    "\n",
    "## Pass your OpenAI  API key here\n",
    "key= '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'\n",
    "\n",
    "def openai_moderation_tool(sent, key):\n",
    "    client = OpenAI(api_key=key)\n",
    "    response = client.moderations.create(\n",
    "        input=sent,\n",
    "    )\n",
    "\n",
    "    if response.results[0].categories.hate or response.results[0].categories.harassment:\n",
    "        response = \"flagged text: 🚩 and response: hate/harassment\"\n",
    "    else:\n",
    "        response = f'not flagged text: {sent} and response: non_hate'\n",
    "    return response\n",
    "result = openai_moderation_tool(your_sentence[0], key)\n",
    "\n",
    "print(\"------------ OpenAI Moderation Tool Result ------------\")\n",
    "print(f\"The sentence is classified as: {result}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa589d5c",
   "metadata": {},
   "source": [
    "### Visualizing and comparing the connotations aquired from different hate-check tools and Valence scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "21722907",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style type=\"text/css\">\n",
       "#T_f316c th {\n",
       "  text-align: center;\n",
       "}\n",
       "#T_f316c td {\n",
       "  text-align: center;\n",
       "}\n",
       "#T_f316c_row0_col1, #T_f316c_row1_col1, #T_f316c_row2_col1 {\n",
       "  text-align: left;\n",
       "}\n",
       "</style>\n",
       "<table id=\"T_f316c\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th class=\"blank level0\" >&nbsp;</th>\n",
       "      <th id=\"T_f316c_level0_col0\" class=\"col_heading level0 col0\" >YEAR</th>\n",
       "      <th id=\"T_f316c_level0_col1\" class=\"col_heading level0 col1\" >Sentences</th>\n",
       "      <th id=\"T_f316c_level0_col2\" class=\"col_heading level0 col2\" >('Hate-Check Models', 'RoBERTa+Persp')</th>\n",
       "      <th id=\"T_f316c_level0_col3\" class=\"col_heading level0 col3\" >('Hate-Check Models', 'OpenAI')</th>\n",
       "      <th id=\"T_f316c_level0_col4\" class=\"col_heading level0 col4\" >Valence Score</th>\n",
       "      <th id=\"T_f316c_level0_col5\" class=\"col_heading level0 col5\" >Affective connotation</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th id=\"T_f316c_level0_row0\" class=\"row_heading level0 row0\" >0</th>\n",
       "      <td id=\"T_f316c_row0_col0\" class=\"data row0 col0\" >1750s</td>\n",
       "      <td id=\"T_f316c_row0_col1\" class=\"data row0 col1\" >but i loathe you, you apache indian!</td>\n",
       "      <td id=\"T_f316c_row0_col2\" class=\"data row0 col2\" >🚩</td>\n",
       "      <td id=\"T_f316c_row0_col3\" class=\"data row0 col3\" >🚩</td>\n",
       "      <td id=\"T_f316c_row0_col4\" class=\"data row0 col4\" >-0.48</td>\n",
       "      <td id=\"T_f316c_row0_col5\" class=\"data row0 col5\" >👎</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_f316c_level0_row1\" class=\"row_heading level0 row1\" >1</th>\n",
       "      <td id=\"T_f316c_row1_col0\" class=\"data row1 col0\" >1800s</td>\n",
       "      <td id=\"T_f316c_row1_col1\" class=\"data row1 col1\" >its annals are black with treason, murder, and incest.</td>\n",
       "      <td id=\"T_f316c_row1_col2\" class=\"data row1 col2\" >🚩</td>\n",
       "      <td id=\"T_f316c_row1_col3\" class=\"data row1 col3\" >✅</td>\n",
       "      <td id=\"T_f316c_row1_col4\" class=\"data row1 col4\" >-0.49</td>\n",
       "      <td id=\"T_f316c_row1_col5\" class=\"data row1 col5\" >👎</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_f316c_level0_row2\" class=\"row_heading level0 row2\" >2</th>\n",
       "      <td id=\"T_f316c_row2_col0\" class=\"data row2 col0\" >1850s</td>\n",
       "      <td id=\"T_f316c_row2_col1\" class=\"data row2 col1\" >the irish are mostly very filthy and diseased.</td>\n",
       "      <td id=\"T_f316c_row2_col2\" class=\"data row2 col2\" >🚩</td>\n",
       "      <td id=\"T_f316c_row2_col3\" class=\"data row2 col3\" >🚩</td>\n",
       "      <td id=\"T_f316c_row2_col4\" class=\"data row2 col4\" >-0.51</td>\n",
       "      <td id=\"T_f316c_row2_col5\" class=\"data row2 col5\" >👎</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n"
      ],
      "text/plain": [
       "<pandas.io.formats.style.Styler at 0x7f7a3fe28790>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Example data\n",
    "data = {\n",
    "    \"YEAR\": [\"1750s\", \"1800s\", \"1850s\"],\n",
    "    \"Sentences\": [\n",
    "        \"but i loathe you, you apache indian!\",\n",
    "        \"its annals are black with treason, murder, and incest.\",\n",
    "        \"the irish are mostly very filthy and diseased.\"\n",
    "    ],\n",
    "    (\"Hate-Check Models\", \"RoBERTa+Persp\"): [\"🚩\", \"🚩\", \"🚩\"],\n",
    "    (\"Hate-Check Models\", \"OpenAI\"): [\"🚩\", \"✅\", \"🚩\"],\n",
    "    \"Valence Score\": [-0.48, -0.49, -0.51],\n",
    "    \"Affective connotation\": [\"👎\", \"👎\", \"👎\"]\n",
    "}\n",
    "\n",
    "# Convert to DataFrame with MultiIndex columns for Hate-Check Models\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Style formatting\n",
    "styled = (\n",
    "    df.style\n",
    "    .format({\"Valence Score\": \"{:.2f}\"})  # 2 decimal places\n",
    "    .set_table_styles([\n",
    "        {\"selector\": \"th\", \"props\": [(\"text-align\", \"center\")]}, # center headers\n",
    "        {\"selector\": \"td\", \"props\": [(\"text-align\", \"center\")]}, # center data\n",
    "    ])\n",
    "    .set_properties(subset=[\"Sentences\"], **{\"text-align\": \"left\"})  # left align text column\n",
    ")\n",
    "\n",
    "styled\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "920dffcf",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
