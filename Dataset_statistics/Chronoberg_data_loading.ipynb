{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3df1e888",
   "metadata": {},
   "source": [
    "### Working with CHRONOBERG \n",
    "\n",
    "In this notebook, we walk through the Chronoberg dataset explaining how to use it to perform lexical analysis on prefered temporal slice and lexical analysis. The dataset is publicly available at [ChronoBerg](https://huggingface.co/datasets/sdp56/ChronoBerg/tree/main).\n",
    "\n",
    "The dataset is made available in two variants: \n",
    "- The non-annotated raw ChronoBerg\n",
    "- The annotated ChronoBerg\n",
    "\n",
    "In each version, the text is grouped by years from 1750-2000s. The annotated version employs a further splitting of texts into sentences. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6a72488a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json, re\n",
    "import os\n",
    "import nltk\n",
    "import pandas as pd \n",
    "from nltk.tokenize import sent_tokenize\n",
    "from nltk.tokenize import sent_tokenize\n",
    "import itertools \n",
    "from tqdm import tqdm\n",
    "from gensim.models.word2vec import Word2Vec\n",
    "import gensim \n",
    "from IPython.display import display, Markdown\n",
    "from gensim.parsing.preprocessing import remove_stopwords, strip_punctuation, strip_multiple_whitespaces, strip_numeric\n",
    "from gensim.parsing.preprocessing import preprocess_string, preprocess_documents"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b24a2988",
   "metadata": {},
   "source": [
    "#### Loading the dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6490f1e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "249it [00:47,  5.26it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data loaded\n",
      "------------------------------------------\n",
      "The number of years in the dataset is:  249\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "data_dict = {'year': [], 'text': []}\n",
    "with open('/app/src/ChronoBerg/cade/data_json/pg_books_historic.jsonl', 'r', encoding='utf-8') as dataset_in:\n",
    "    for line in tqdm(dataset_in):\n",
    "        file = json.loads(line)\n",
    "        text = file['text']\n",
    "        text = text.replace('\\n', ' ')\n",
    "        # Remove all kinds of quotation marks\n",
    "        file['text'] = text\n",
    "        data_dict['text'].append(file['text'])\n",
    "        data_dict['year'].append(file['year'])\n",
    "\n",
    "print('data loaded')\n",
    "\n",
    "print(\"------------------------------------------\")\n",
    "print(\"The number of years in the dataset is: \", len(set(data_dict['year'])) )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64420a92",
   "metadata": {},
   "source": [
    "### Visualize sample textual data from a particular year"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "bbd69088",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "** Time_Interval- 1900**:  [Illustration]     The Wonderful Wizard of Oz  by L. Frank Baum   This book is dedicated to my good friend & comrade My Wife L.F.B.   Contents   Introduction  Chapter I. The Cyclone  Chapter II. The Council with the Munchkins  Chapter III. How Dorothy Saved the Scarecrow  Chapter IV. The Road Through the Forest  Chapter V. The Rescue of the Tin Woodman  Chapter VI.  The Cowardly Lion  Chapter VII. The Journey to the Great Oz  Chapter VIII. The Deadly Poppy Field  Chapter IX. The Queen of the Field Mice  Chapter X. The Guardian of the Gates  Chapter XI. The Emerald City of Oz  Chapter XII. The Search for the Wicked Witch  Chapter XIII. The Rescue  Chapter XIV. The Winged Monkeys  Chapter XV. The Discovery of Oz, the Terrible  Chapter XVI. The Magic Art of the Great Humbug  Chapter XVII. How the Balloon Was Launched  Chapter XVIII. Away to the South  Chapter XIX. Attacked by the Fighting Trees  Chapter XX. The Dainty China Country  Chapter XXI. The Lion Becomes the King of Beasts  Chapter XXII. The Country of the Quadlings  Chapter XXIII. Glinda The Good Witch Grants Dorothyâs Wish  Chapter XXIV. Home Again     Introduction   Folklore, legends, myths and fairy tales have followed childhood through the ages, for every healthy youngster has a wholesome and instinctive love for stories fantastic, marvelous and manifestly unreal. The winged fairies of Grimm and Andersen have brought more happiness to childish hearts than all other human creations.  Yet the old time fairy tale, h"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#### Show a sample text from a particular year\n",
    "year = 1900\n",
    "for i in range(len(data_dict['year'])):\n",
    "    if data_dict['year'][i] == year:\n",
    "        display(Markdown(f\"** Time_Interval- {data_dict['year'][i]}**: {data_dict['text'][i][:1500]}\"))\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d6acd7af",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Short the text by year\n",
    "df = pd.DataFrame(data_dict)\n",
    "df.head()\n",
    "df = df.sort_values(by=['year'], ascending=True)\n",
    "\n",
    "### Recreate the data_dict based on the sorted dataframe\n",
    "data_dict = {'year': [], 'text': []}\n",
    "for i in range(len(df)):\n",
    "    data_dict['year'].append(df['year'].iloc[i])\n",
    "    data_dict['text'].append(df['text'].iloc[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7514e0df",
   "metadata": {},
   "source": [
    "#### Transform the block of text into a set of sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "cadf18e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 249/249 [35:31<00:00,  8.56s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total sentences: 88957134\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "text_total = []\n",
    "for text in tqdm(data_dict['text']):\n",
    "    sentence = sent_tokenize(text)\n",
    "    text_total.append(sentence)\n",
    "\n",
    "text_total = list(itertools.chain(*text_total))\n",
    "print(f'Total sentences: {len(text_total)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7a243ea",
   "metadata": {},
   "source": [
    "### Extract Textual data from a particular year "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "de4c43fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of sentences in year 1771: 10323\n",
      "------------------------------ Sample sentences ------------------------------\n",
      "Sentence 1:       THE EXPEDITION OF HUMPHRY CLINKER  by TOBIAS SMOLLETT     To Mr HENRY DAVIS, Bookseller, in London.\n",
      "Sentence 2: ABERGAVENNY, Aug. 4.\n",
      "Sentence 3: RESPECTED SIR,  I have received your esteemed favour of the 13th ultimo, whereby it appeareth, that you have perused those same Letters, the which were delivered unto you by my friend, the reverend Mr Hugo Behn; and I am pleased to find you think they may be printed with a good prospect of success; in as much as the objections you mention, I humbly conceive, are such as may be redargued, if not entirely removed--And, first, in the first place, as touching what prosecutions may arise from printing the private correspondence of persons still living, give me leave, with all due submission, to observe, that the Letters in question were not written and sent under the seal of secrecy; that they have no tendency to the mala fama, or prejudice of any person whatsoever; but rather to the information and edification of mankind: so that it becometh a sort of duty to promulgate them in usum publicum.\n",
      "Sentence 4: Besides, I have consulted Mr Davy Higgins, an eminent attorney of this place, who, after due inspection and consideration, declareth, That he doth not think the said Letters contain any matter which will be held actionable in the eye of the law.\n",
      "Sentence 5: Finally, if you and I should come to a right understanding, I do declare in verbo sacerdotis, that, in case of any such prosecution, I will take the whole upon my own shoulders, even quoad fine and imprisonment, though, I must confess, I should not care to undergo flagellation: Tam ad turpitudinem, quam ad amaritudinem poenoe spectans--Secondly, concerning the personal resentment of Mr Justice Lismahago, I may say, non flocci facio--I would not willingly vilipend any Christian, if, peradventure, he deserveth that epithet: albeit, I am much surprised that more care is not taken to exclude from the commission all such vagrant foreigners as may be justly suspected of disaffection to our happy constitution, in church and state--God forbid that I should be so uncharitable, as to affirm, positively, that the said Lismahago is no better than a Jesuit in disguise; but this I will assert and maintain, totis viribus, that, from the day he qualified, he has never been once seen intra templi parietes, that is to say, within the parish church.\n"
     ]
    }
   ],
   "source": [
    "def extract_text_by_year(year, data_dict):\n",
    "    sents = []\n",
    "    texts = data_dict['text'][year]\n",
    "    sents.extend(sent_tokenize(texts))\n",
    "    return sents, texts\n",
    "\n",
    "#### Transform the block of text into a set of sentences\n",
    "year = 20\n",
    "sents, texts = extract_text_by_year(year, data_dict)\n",
    "print(f'Number of sentences in year {data_dict[\"year\"][year]}: {len(sents)}')\n",
    "print(\"------------------------------ Sample sentences ------------------------------\")\n",
    "for i in range(len(sents[:5])):\n",
    "    print(f'Sentence {i+1}: {sents[i]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "626b907e",
   "metadata": {},
   "source": [
    "#### Polish the text if needed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c996191f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_text(texts):\n",
    "    for i in range(len(texts)):\n",
    "        text = texts[i]\n",
    "        text = re.sub(r'[\\'\\\"]', '', text)\n",
    "        text = re.sub('\\.', ' ', text)\n",
    "        text = re.sub(r'[\\x80-\\xFF]', '', text)\n",
    "        text = re.sub(r'\\d+','', text) \n",
    "        # Reduce all consecutive whitespace to a single whitespace\n",
    "        text = re.sub(r'\\s+', ' ', text)\n",
    "        texts[i] = text\n",
    "    return texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "54477090",
   "metadata": {},
   "outputs": [],
   "source": [
    "texts_ = preprocess_text(sents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f091a872",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
